{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fa9a98-80b8-4253-b0dc-eb6e83396292",
   "metadata": {},
   "source": [
    "# Lab 6: Multi-Agent Orchestration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bfb098-cd91-4724-b5e7-80851e0acc2a",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734aeef-4811-404a-980b-4921dbaaaa4c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b>Access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b069b-8d62-4b63-9842-b2c64191f640",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI models can vary with each execution due to their dynamic, probabilistic nature. Your results may differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3d0e1e-6ce1-4015-88e0-7e0cc0b8acc0",
   "metadata": {},
   "source": [
    "Letta agents persist information over time and restarts by saving data to a database. These lessons do not require past information. To enable a clean restart, the database is cleared before starting the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27d6c7-4bc5-4033-8af1-6b67c8e061d0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "!rm  -f ~/.letta/sqlite.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d49442-0a34-4f6b-9ecb-acd80f2ba80f",
   "metadata": {},
   "source": [
    "## Section 0: Setup a client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e108e4-bcf9-4be1-b2c0-66539cbe9ec3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from helper import nb_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0e6729-0f02-497f-8f85-688ed4f65f68",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Config:  /home/jovyan/.letta/config\n",
      "ðŸ“– Letta configuration file updated!\n",
      "ðŸ§  model\t-> gpt-4\n",
      "ðŸ–¥ï¸  endpoint\t-> http://jupyter-api-proxy.internal.dlai/rev-proxy/letta\n",
      "Saved Config:  /home/jovyan/.letta/config\n",
      "Saved Config:  /home/jovyan/.letta/config\n"
     ]
    }
   ],
   "source": [
    "from letta import create_client \n",
    "\n",
    "client = create_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bbb19b-2fa2-4cac-8803-5cb179c36d47",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from letta.schemas.llm_config import LLMConfig\n",
    "\n",
    "client.set_default_llm_config(LLMConfig.default_config(\"gpt-4o-mini\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef3345-8dd7-4ddc-8f8e-c6a225c64a34",
   "metadata": {},
   "source": [
    "## Section 1: Shared Memory Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87954850-4566-45e8-b52e-610fe7de8adf",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from letta.schemas.block import Block \n",
    "\n",
    "org_description= \"The company is called AgentOS \" \\\n",
    "+ \"and is building AI tools to make it easier to create \" \\\n",
    "+ \"and deploy LLM agents.\"\n",
    "\n",
    "org_block = Block(name=\"company\", value=org_description )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914befec-6157-4da0-94e4-beb40897062e",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from letta.schemas.memory import BasicBlockMemory\n",
    "\n",
    "class OrgMemory(BasicBlockMemory): \n",
    "\n",
    "    def __init__(self, persona: str, org_block: Block): \n",
    "        persona_block = Block(name=\"persona\", value=persona)\n",
    "        super().__init__(blocks=[persona_block, org_block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ae6cb1-239e-41fc-969a-8efff752969b",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "#cleanup. This code will remove agents if the code is run more than once otherwise it will do nothing.\n",
    "for agent in client.list_agents(): \n",
    "    client.delete_agent(agent.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c540a-a96f-440e-83b2-6a7478d95f7d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# client.delete_agent(recruiter_agent.id)  #cleanup code for repeated runs if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c85a5e-9576-4966-8dc5-19ae8ef09fd5",
   "metadata": {},
   "source": [
    "## Section 2: Orchestrating Multiple Agents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888d64d-e3aa-4a35-90f4-6b5a8c78239e",
   "metadata": {},
   "source": [
    "#### Evaluator Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7b9f35-8599-4465-952a-b2db2d7b1533",
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "def read_resume(self, name: str): \n",
    "    \"\"\"\n",
    "    Read the resume data for a candidate given the name\n",
    "\n",
    "    Args: \n",
    "        name (str): Candidate name \n",
    "\n",
    "    Returns: \n",
    "        resume_data (str): Candidate's resume data \n",
    "    \"\"\"\n",
    "    import os\n",
    "    filepath = os.path.join(\"data\", \"resumes\", name.lower().replace(\" \", \"_\") + \".txt\")\n",
    "    #print(\"read\", filepath)\n",
    "    return open(filepath).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2410745b-73f3-4d4e-a369-8c6fc19cd752",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "def submit_evaluation(\n",
    "    self, \n",
    "    candidate_name: str, \n",
    "    reach_out: bool, \n",
    "    resume: str, \n",
    "    justification: str\n",
    "): \n",
    "    \"\"\"\n",
    "    Submit a candidate for outreach. \n",
    "\n",
    "    Args: \n",
    "        candidate_name (str): The name of the candidate\n",
    "        reach_out (bool): Whether to reach out to the candidate\n",
    "        resume (str): The text representation of the candidate's resume \n",
    "        justification (str): Justification for reaching out or not\n",
    "    \"\"\"\n",
    "    from letta import create_client \n",
    "    client = create_client()\n",
    "\n",
    "    message = \"Reach out to the following candidate. \" \\\n",
    "    + f\"Name: {candidate_name}\\n\" \\\n",
    "    + f\"Resume Data: {resume}\\n\" \\\n",
    "    + f\"Justification: {justification}\"\n",
    "    print(\"eval agent\", candidate_name)\n",
    "    if reach_out:\n",
    "        response = client.send_message(\n",
    "            agent_name=\"outreach_agent\", \n",
    "            role=\"user\", \n",
    "            message=message\n",
    "        ) \n",
    "    else: \n",
    "        print(f\"Candidate {candidate_name} is rejected: {justification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d419bd-020c-4f6f-81c3-e704d65f5306",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "read_resume_tool = client.create_tool(read_resume) \n",
    "submit_evaluation_tool = client.create_tool(submit_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df27d7b-d2b6-4d87-90c3-6759a70fa8d7",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "skills = \"Front-end (React, Typescript), software engineering \" \\\n",
    "+ \"(ideally Python), and experience with LLMs.\"\n",
    "eval_persona = f\"You are responsible to finding good recruiting \" \\\n",
    "+ \"candidates, for the company description. \" \\\n",
    "+ f\"Ideal canddiates have skills: {skills}. \" \\\n",
    "+ \"Submit your candidate evaluation with the submit_evaluation tool. \"\n",
    "\n",
    "\n",
    "eval_agent = client.create_agent(\n",
    "    name=\"eval_agent\", \n",
    "    memory=OrgMemory(\n",
    "        persona=eval_persona, \n",
    "        org_block=org_block,\n",
    "    ), \n",
    "    tools=[read_resume_tool.name, submit_evaluation_tool.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c675a-13a6-4452-bbde-8bdf73bf3d3b",
   "metadata": {},
   "source": [
    "#### Outreach agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f44ebefc-ba31-4901-acf4-893ffe624fb5",
   "metadata": {
    "height": 200
   },
   "outputs": [],
   "source": [
    "def email_candidate(self, content: str): \n",
    "    \"\"\"\n",
    "    Send an email\n",
    "\n",
    "    Args: \n",
    "        content (str): Content of the email \n",
    "    \"\"\"\n",
    "    print(\"Pretend to email:\", content)\n",
    "    return\n",
    "\n",
    "email_candidate_tool = client.create_tool(email_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4d7b41-e563-4785-867a-02bd6127ceea",
   "metadata": {
    "height": 472
   },
   "outputs": [],
   "source": [
    "outreach_persona = \"You are responsible for sending outbound emails \" \\\n",
    "+ \"on behalf of a company with the send_emails tool to \" \\\n",
    "+ \"potential candidates. \" \\\n",
    "+ \"If possible, make sure to personalize the email by appealing \" \\\n",
    "+ \"to the recipient with details about the company. \" \\\n",
    "+ \"You position is `Head Recruiter`, and you go by the name Bob, with contact info bob@gmail.com. \" \\\n",
    "+ \"\"\"\n",
    "Follow this email template: \n",
    "\n",
    "Hi <candidate name>, \n",
    "\n",
    "<content> \n",
    "\n",
    "Best, \n",
    "<your name> \n",
    "<company name> \n",
    "\"\"\"\n",
    "\n",
    "outreach_agent = client.create_agent(\n",
    "    name=\"outreach_agent\", \n",
    "    memory=OrgMemory(\n",
    "        persona=outreach_persona, \n",
    "        org_block=org_block\n",
    "    ), \n",
    "    tools=[email_candidate_tool.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2534c3f7-55d9-4f5f-892c-82221ccd1b78",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval agent Tony Stark\n",
      "Pretend to email: Hi Tony Stark,\n",
      "\n",
      "I came across your profile and was impressed by your substantial expertise in Front-end development and remarkable academic background from MIT. I represent AgentOS, a company dedicated to creating and deploying LLM agents. We believe your React development skills could greatly aid our team.\n",
      "\n",
      "If this opportunity interests you, I'd love to arrange a chat to discuss this potential fit further.\n",
      "\n",
      "Best,\n",
      "Bob\n",
      "AgentOS\n"
     ]
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_name=\"eval_agent\", \n",
    "    role=\"user\", \n",
    "    message=\"Candidate: Tony Stark\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ddb91c-5d8c-4f1c-87fd-f8c2b75f7990",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">A candidate has been proposed: Tony Stark. Let&#x27;s read the resume and evaluate his fit for AgentOS. It&#x27;s exciting being part of the hiring process!</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">read_resume</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"name\"</span>: <span class=\"json-key\">\"Tony Stark\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION RETURN</div>\n",
       "            <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"Tony Stark\\nFrontend Engineer - React Specialist\\nBoston, MA | (123) 456-7890 | tony.stark@email.com | LinkedIn: /in/tonystark\\n\\nSummary\\nInnovative Frontend Engineer with a Bachelorâ€™s degree in Computer Science from the Massachusetts Institute of Technology (MIT) and over 6 years of professional experience in building and optimizing user-focused web applications. Adept at using React to create efficient, scalable, and dynamic client-side applications.\\n\\nEducation\\nMassachusetts Institute of Technology (MIT)\\nBachelor of Science in Computer Science\\nGraduated: <span class=\"json-number\">2018</span>\\n\\nGPA: <span class=\"json-number\">4</span>.0\\nDeanâ€™s List; Received the MIT Undergraduate Research Opportunities Program (UROP) grant.\\nProfessional Experience\\nSenior Frontend Developer\\nInnovatech Solutions, Boston, MA\\nJune 2020 - Present\\n\\nLead a team of five developers in the design and implementation of a React-based web application for a leading financial services firm, improving user engagement by 40%.\\nImplemented advanced features in React, including Hooks, Context API, and Redux for state management, leading to a 30% reduction in loading times.\\nSpearheaded the migration from AngularJS to React, including updating the architecture and optimizing legacy codebases.\\nFrontend Developer\\nTechGiant Inc, Boston, MA\\nJuly 2018 - May 2020\\n\\nDeveloped new user-facing features using React.js and integrated applications with backend services via RESTful APIs.\\nCollaborated with UX/UI designers to translate designs and wireframes into high-quality code.\\nOptimized components for maximum performance across a vast array of web-capable devices and browsers.\\nConducted comprehensive debugging using React Developer Tools.\\nSkills\\nLanguages: JavaScript (ES6+), HTML5, CSS3, TypeScript\\nFrameworks/Libraries: React, Redux, Next.js, Node.js\\nTools: Webpack, Babel, NPM, Jest, Enzyme\\nOthers: Responsive Design, RESTful Services, Agile Methodologies\\nProjects\\nReal-Time Analytics Dashboard\\nFrontend Developer\\n\\nDeveloped a real-time analytics dashboard using React and Redux, which supported WebSocket for real-time updates.\\nImplemented various D3.js visualizations integrated within the React environment for dynamic data presentation.\\nE-Commerce Platform Revamp\\nSenior Frontend Developer\\n\\nLed the frontend development of a full-scale revamp for a major e-commerce platform, improving the site's performance and user experience.\\nIntegrated Stripe for payment processing and managed state with Redux-Saga.\\nCertifications\\nCertified React Developer - React Training Institute, 2019\\nAgile Certified Practitioner (PMI-ACP) â€“ Project Management Institute, 2020\\nInterests\\nAugmented Reality Development\\nOpen Source Contribution\\nRobotics\\nTony's background and experience are designed to showcase a strong skill set in React development and an impressive academic background from MIT.\\n\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 11:10:57 AM UTC+0000\"</span><br>}</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">Tony Stark... Now here&#x27;s a resume! Bachelor&#x27;s degree from MIT, React specialist, frontend experience, led teams, improved performance, even has Agile certification. Let&#x27;s document these key details. They might come in handy.</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">archival_memory_insert</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"content\"</span>: <span class=\"json-key\">\"Candidate: Tony Stark. Profession: Frontend Engineer. Skills: React, JavaScript, HTML5, CSS3, TypeScript, Redux, Next.js, Node.js, and Agile Methodologies. Experience: Senior Frontend Developer at Innovatech Solutions, Frontend Developer at TechGiant Inc. Education: Bachelor's degree in Computer Science from MIT. Certifications: Certified React Developer, Agile Certified Practitioner.\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">Alright, let&#x27;s assess this candidate against AgentOS&#x27;s requirements. Strong frontend experience with proficiency in React and TypeScript, check. Software engineering experience, check. Now, does he have experience with LLMs? I don&#x27;t see that explicitly... Let&#x27;s analyze this further.</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">archival_memory_search</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"query\"</span>: <span class=\"json-key\">\"Tony Stark LLM experience\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION RETURN</div>\n",
       "            <div class=\"content\">{<br>&nbsp;&nbsp;<span class=\"json-key\">\"status\"</span>: <span class=\"json-key\">\"OK\",<br>&nbsp;&nbsp;\"message\"</span>: <span class=\"json-key\">\"Showing 1 of 1 results (page 0/0): [\\n&nbsp;&nbsp;\\\"timestamp: <span class=\"json-number\">2024</span>-11-13 11:11:10 AM UTC+0000, memory: Candidate: Tony Stark. Profession: Frontend Engineer. Skills: React, JavaScript, HTML5, CSS3, TypeScript, Redux, Next.js, Node.js, and Agile Methodologies. Experience: Senior Frontend Developer at Innovatech Solutions, Frontend Developer at TechGiant Inc. Education: Bachelor's degree in Computer Science from MIT. Certifications: Certified React Developer, Agile Certified Practitioner.\\\"\\n]\",<br>&nbsp;&nbsp;\"time\"</span>: <span class=\"json-string\">\"2024-11-13 11:11:10 AM UTC+0000\"</span><br>}</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">It seems Tony Stark doesn&#x27;t possess any explicit experience with LLMs as per the archival memory search. His impressive skills and experience certainly fit the bill otherwise. Maybe his agility and strong foundation would enable him to gain proficiency in LLMs quickly. Let&#x27;s evaluate for recommendation.</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">submit_evaluation</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"candidate_name\"</span>: <span class=\"json-key\">\"Tony Stark\",<br>&nbsp;&nbsp;\"reach_out\"</span>: <span class=\"json-boolean\">true</span>,<br>&nbsp;&nbsp;<span class=\"json-key\">\"resume\"</span>: <span class=\"json-key\">\"Tony Stark\\nFrontend Engineer - React Specialist\\nBoston, MA | (123) 456-7890 | tony.stark@email.com | LinkedIn: /in/tonystark\\n\\nSummary\\nInnovative Frontend Engineer with a Bachelorâ€™s degree in Computer Science from the Massachusetts Institute of Technology (MIT) and over 6 years of professional experience in building and optimizing user-focused web applications. Adept at using React to create efficient, scalable, and dynamic client-side applications.\\n\\nEducation\\nMassachusetts Institute of Technology (MIT)\\nBachelor of Science in Computer Science\\nGraduated: <span class=\"json-number\">2018</span>\\n\\nGPA: <span class=\"json-number\">4</span>.0\\nDeanâ€™s List; Received the MIT Undergraduate Research Opportunities Program (UROP) grant.\\nProfessional Experience\\nSenior Frontend Developer\\nInnovatech Solutions, Boston, MA\\nJune 2020 - Present\\n\\nLead a team of five developers in the design and implementation of a React-based web application for a leading financial services firm, improving user engagement by 40%.\\nImplemented advanced features in React, including Hooks, Context API, and Redux for state management, leading to a 30% reduction in loading times.\\nSpearheaded the migration from AngularJS to React, including updating the architecture and optimizing legacy codebases.\\nFrontend Developer\\nTechGiant Inc, Boston, MA\\nJuly 2018 - May 2020\\n\\nDeveloped new user-facing features using React.js and integrated applications with backend services via RESTful APIs.\\nCollaborated with UX/UI designers to translate designs and wireframes into high-quality code.\\nOptimized components for maximum performance across a vast array of web-capable devices and browsers.\\nConducted comprehensive debugging using React Developer Tools.\\nSkills\\nLanguages: JavaScript (ES6+), HTML5, CSS3, TypeScript\\nFrameworks/Libraries: React, Redux, Next.js, Node.js\\nTools: Webpack, Babel, NPM, Jest, Enzyme\\nOthers: Responsive Design, RESTful Services, Agile Methodologies\\nProjects\\nReal-Time Analytics Dashboard\\nFrontend Developer\\n\\nDeveloped a real-time analytics dashboard using React and Redux, which supported WebSocket for real-time updates.\\nImplemented various D3.js visualizations integrated within the React environment for dynamic data presentation.\\nE-Commerce Platform Revamp\\nSenior Frontend Developer\\n\\nLed the frontend development of a full-scale revamp for a major e-commerce platform, improving the site's performance and user experience.\\nIntegrated Stripe for payment processing and managed state with Redux-Saga.\\nCertifications\\nCertified React Developer - React Training Institute, 2019\\nAgile Certified Practitioner (PMI-ACP) â€“ Project Management Institute, 2020\\nInterests\\nAugmented Reality Development\\nOpen Source Contribution\\nRobotics\\nTony's background and experience are designed to showcase a strong skill set in React development and an impressive academic background from MIT.\\n\",<br>&nbsp;&nbsp;\"justification\"</span>: <span class=\"json-key\">\"Tony Stark has extensive experience with Front-end development, React, and TypeScript, and also possesses good software engineering skills. He doesn't have explicit experience with LLMs, but given his software engineering background and agility reflected from his experience, it's likely he can gain proficiency quickly. His professional and academic credentials are impressive, making him a viable candidate for AgentOS.\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">And... evaluation submitted! Now, we wait to hear back. I must say, it&#x27;s exhilarating being part of the talent acquisition process for AgentOS. Onward to the next candidate!</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"Candidate evaluation for Tony Stark has been submitted! I'm ready for the next candidate.\"</span><br>})</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc0a070-d3e5-4832-90af-99d3544862e2",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "feedback = \"Our company pivoted to foundation model training\"\n",
    "response = client.send_message(\n",
    "    agent_name=\"eval_agent\", \n",
    "    role=\"user\", \n",
    "    message=feedback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "356e2cb1-a89a-4e56-8bed-1aabfd92bb09",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "feedback = \"The company is also renamed to FoundationAI\"\n",
    "response = client.send_message(\n",
    "    agent_name=\"eval_agent\", \n",
    "    role=\"user\", \n",
    "    message=feedback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e64ee5-fc01-436c-839a-73e65fb162c9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .message-container {\n",
       "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "            max-width: 800px;\n",
       "            margin: 20px auto;\n",
       "            background-color: #1e1e1e;\n",
       "            border-radius: 8px;\n",
       "            overflow: hidden;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .message {\n",
       "            padding: 10px 15px;\n",
       "            border-bottom: 1px solid #3a3a3a;\n",
       "        }\n",
       "        .message:last-child {\n",
       "            border-bottom: none;\n",
       "        }\n",
       "        .title {\n",
       "            font-weight: bold;\n",
       "            margin-bottom: 5px;\n",
       "            color: #ffffff;\n",
       "            text-transform: uppercase;\n",
       "            font-size: 0.9em;\n",
       "        }\n",
       "        .content {\n",
       "            background-color: #2d2d2d;\n",
       "            border-radius: 4px;\n",
       "            padding: 5px 10px;\n",
       "            font-family: 'Consolas', 'Courier New', monospace;\n",
       "            white-space: pre-wrap;\n",
       "        }\n",
       "        .status-line {\n",
       "            margin-bottom: 5px;\n",
       "            color: #d4d4d4;\n",
       "        }\n",
       "        .function-name { color: #569cd6; }\n",
       "        .json-key { color: #9cdcfe; }\n",
       "        .json-string { color: #ce9178; }\n",
       "        .json-number { color: #b5cea8; }\n",
       "        .json-boolean { color: #569cd6; }\n",
       "        .internal-monologue { font-style: italic; }\n",
       "    </style>\n",
       "    <div class=\"message-container\">\n",
       "    \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">FoundationAI, what a fitting name for our new direction! I&#x27;ll make sure this name change is reflected in my core memory. This is quite the pivotal moment for us!</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">core_memory_replace</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"name\"</span>: <span class=\"json-key\">\"company\",<br>&nbsp;&nbsp;\"old_content\"</span>: <span class=\"json-key\">\"The company is called AgentOS and is building AI tools to make it easier to create and deploy LLM agents.\",<br>&nbsp;&nbsp;\"new_content\"</span>: <span class=\"json-key\">\"The company is now called FoundationAI, focusing on foundation model training.\",<br>&nbsp;&nbsp;\"request_heartbeat\"</span>: <span class=\"json-boolean\">true</span><br>})</div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">INTERNAL MONOLOGUE</div>\n",
       "            <div class=\"content\"><span class=\"internal-monologue\">Our transformation to FoundationAI is now etched in my core memory. I&#x27;m ready to continue scoping out the best talent for FoundationAI&#x27;s new trajectory. Onward and upward!</span></div>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"message\">\n",
       "            <div class=\"title\">FUNCTION CALL</div>\n",
       "            <div class=\"content\"><span class=\"function-name\">send_message</span>({<br>&nbsp;&nbsp;<span class=\"json-key\">\"message\"</span>: <span class=\"json-string\">\"The company's name has been updated to FoundationAI! I'm ready to continue our search for the best talent to join our evolving mission.\"</span><br>})</div>\n",
       "        </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228ba181-b0bf-4f04-bf94-1e520caeafc3",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval agent Spongebob Squarepants\n",
      "Pretend to email: Hi Spongebob Squarepants,\n",
      "\n",
      "Your esteemed background as an AI researcher with a specialization in agent technology caught our attention. In particular, your extensive knowledge with Python, machine learning, and multi-agent systems fits well with our objectives at FoundationAI.\n",
      "\n",
      "We are currently working on projects that would greatly benefit from your expertise. If you find this opportunity engaging, it would be great to arrange a discussion to explore this further.\n",
      "\n",
      "Best,\n",
      "Bob\n",
      "FoundationAI\n"
     ]
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_name=\"eval_agent\", \n",
    "    role=\"system\", \n",
    "    message=\"Candidate: Spongebob Squarepants\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdcc39c5-311d-446e-9562-fa88eb2a10d1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(value='The company is now called FoundationAI, focusing on foundation model training.', limit=2000, name='company', template=False, label='company', description=None, metadata_={}, user_id=None, id='block-5b3e0685-6d1e-4ac1-9007-ff7503c21475')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_core_memory(outreach_agent.id).get_block(\"company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2fffc-0dbd-44f4-b90c-b9e5c7d4cddf",
   "metadata": {},
   "source": [
    "## Section 3: Adding an orchestrator agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3219788-1cdc-4a85-af9f-b08005fabd28",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "#re-create agents \n",
    "client.delete_agent(eval_agent.id)\n",
    "client.delete_agent(outreach_agent.id)\n",
    "\n",
    "eval_agent = client.create_agent(\n",
    "    name=\"eval_agent\", \n",
    "    memory=OrgMemory(\n",
    "        persona=eval_persona, \n",
    "        org_block=org_block,\n",
    "    ), \n",
    "    tools=[read_resume_tool.name, submit_evaluation_tool.name]\n",
    ")\n",
    "\n",
    "outreach_agent = client.create_agent(\n",
    "    name=\"outreach_agent\", \n",
    "    memory=OrgMemory(\n",
    "        persona=outreach_persona, \n",
    "        org_block=org_block\n",
    "    ), \n",
    "    tools=[email_candidate_tool.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e1dd11c-31a6-4d8e-b713-c6bab84ddea1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(value='The company is now called FoundationAI, focusing on foundation model training.', limit=2000, name='company', template=False, label='company', description=None, metadata_={}, user_id='user-1682ee8d-779f-45dd-a247-7659f36d845a', id='block-5b3e0685-6d1e-4ac1-9007-ff7503c21475')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_block(org_block.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abce960f-aaad-459c-82fc-c0c0e50543c1",
   "metadata": {
    "height": 676
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def search_candidates_db(self, page: int) -> Optional[str]: \n",
    "    \"\"\"\n",
    "    Returns 1 candidates per page. \n",
    "    Must start at page 0.\n",
    "    Page 0 returns the first 1 candidate, \n",
    "    Page 1 returns the next 1, etc.\n",
    "    Returns `None` if no candidates remain. \n",
    "\n",
    "    Args: \n",
    "        page (int): The page number to return candidates from \n",
    "\n",
    "    Returns: \n",
    "        candidate_names (List[str]): Names of the candidates\n",
    "    \"\"\"\n",
    "    \n",
    "    names = [\"Tony Stark\", \"Spongebob Squarepants\", \"Gautam Fang\"]\n",
    "    if page >= len(names): \n",
    "        return None\n",
    "    return names[page]\n",
    "\n",
    "def consider_candidate(self, name: str): \n",
    "    \"\"\"\n",
    "    Submit a candidate for consideration. \n",
    "\n",
    "    Args: \n",
    "        name (str): Candidate name to consider \n",
    "    \"\"\"\n",
    "    from letta import create_client \n",
    "    client = create_client()\n",
    "    message = f\"Consider candidate {name}\" \n",
    "    print(\"Sending message to eval agent: \", message)\n",
    "    response = client.send_message(\n",
    "        agent_name=\"eval_agent\", \n",
    "        role=\"user\", \n",
    "        message=message\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f066b8-77dc-4fcf-8c2c-f280338d8e75",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "search_candidate_tool = client.create_tool(search_candidates_db)\n",
    "consider_candidate_tool = client.create_tool(consider_candidate)\n",
    "\n",
    "# create recruiter agent\n",
    "recruiter_agent = client.create_agent(\n",
    "    name=\"recruiter_agent\", \n",
    "    memory=OrgMemory(\n",
    "        persona=\"You run a recruiting process for a company. \" \\\n",
    "        + \"Your job is to continue to pull candidates from the \" \n",
    "        + \"`search_candidates_db` tool until there are no more \" \\\n",
    "        + \"candidates left. \" \\\n",
    "        + \"For each candidate, consider the candidate by calling \"\n",
    "        + \"the `consider_candidate` tool. \" \\\n",
    "        + \"You should continue to call `search_candidates_db` \" \\\n",
    "        + \"followed by `consider_candidate` until there are no more \" \\\n",
    "        \" candidates. Start at page 0. \",\n",
    "        org_block=org_block\n",
    "    ), \n",
    "    tools=[search_candidate_tool.name, consider_candidate_tool.name]\n",
    ")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9d7ebbe-12f3-4949-bc89-0dc1e3b85c42",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending message to eval agent:  Consider candidate Tony Stark\n",
      "eval agent Tony Stark\n",
      "Pretend to email: Hi Tony Stark, \n",
      "\n",
      "I hope this message finds you well. I'm writing to you from FoundationAI, where we specialize in foundation model training. Having reviewed your resume, we were impressed by your React expertise and your extensive background in software development from a prestigious institution like MIT. Your work in React, coupled with your leadership skills and experience from Innovatech Solutions and TechGiant Inc, is precisely the kind of talent we're enthusiastic to bring on board. \n",
      "\n",
      "I'd like to invite you to further discuss your interests and how your experience could be a potential fit for our dynamic team. Do let me know a suitable time for this discussion, and we can schedule a call accordingly. \n",
      "\n",
      "Looking forward to hearing from you, Tony. \n",
      "\n",
      "Best, \n",
      "Bob \n",
      "FoundationAI\n",
      "Sending message to eval agent:  Consider candidate Spongebob Squarepants\n",
      "eval agent Spongebob Squarepants\n",
      "Candidate Spongebob Squarepants is rejected: While Spongebob has a strong background in AI and machine learning, his skills don't meet the front-end requirements (React, Typescript) and LLMs experience that the role demands.\n",
      "Warning: Got a rate limit error ('429 Client Error: Too Many Requests for url: http://jupyter-api-proxy.internal.dlai/rev-proxy/letta/chat/completions') on LLM backend request, waiting 2s then retrying...\n",
      "Warning: Got a rate limit error ('429 Client Error: Too Many Requests for url: http://jupyter-api-proxy.internal.dlai/rev-proxy/letta/chat/completions') on LLM backend request, waiting 8s then retrying...\n",
      "Warning: Got a rate limit error ('429 Client Error: Too Many Requests for url: http://jupyter-api-proxy.internal.dlai/rev-proxy/letta/chat/completions') on LLM backend request, waiting 30s then retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/llm_api/llm_api_tools.py:197\u001b[0m, in \u001b[0;36mretry_with_exponential_backoff.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m http_err:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Retry on specified errors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/llm_api/llm_api_tools.py:329\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(llm_config, messages, user_id, functions, functions_python, function_call, first_message, use_tool_naming, stream, stream_inferface, inner_thoughts_in_kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_chat_completions_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_endpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# https://api.openai.com/v1 -> https://api.openai.com/v1/chat/completions\u001b[39;49;00m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_completion_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/llm_api/openai.py:432\u001b[0m, in \u001b[0;36mopenai_chat_completions_request\u001b[0;34m(url, api_key, chat_completion_request)\u001b[0m\n\u001b[1;32m    431\u001b[0m     printd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot HTTPError, exception=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, payload=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m http_err\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m req_err:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# Handle other requests-related errors (e.g., connection error)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/llm_api/openai.py:422\u001b[0m, in \u001b[0;36mopenai_chat_completions_request\u001b[0;34m(url, api_key, chat_completion_request)\u001b[0m\n\u001b[1;32m    421\u001b[0m printd(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, response.text = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 422\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raises HTTPError for 4XX/5XX status\u001b[39;00m\n\u001b[1;32m    424\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()  \u001b[38;5;66;03m# convert to dict from string\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: http://jupyter-api-proxy.internal.dlai/rev-proxy/letta/chat/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecruiter_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRun generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/client/client.py:1732\u001b[0m, in \u001b[0;36mLocalClient.send_message\u001b[0;34m(self, message, role, agent_id, agent_name, stream_steps, stream_tokens, include_full_message)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1732\u001b[0m     usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1734\u001b[0m     usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39muser_message(user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_id, agent_id\u001b[38;5;241m=\u001b[39magent_id, message\u001b[38;5;241m=\u001b[39mmessage)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/server/server.py:603\u001b[0m, in \u001b[0;36mSyncServer.system_message\u001b[0;34m(self, user_id, agent_id, message, timestamp)\u001b[0m\n\u001b[1;32m    600\u001b[0m     message\u001b[38;5;241m.\u001b[39mcreated_at \u001b[38;5;241m=\u001b[39m timestamp\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Run the agent state forward\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackaged_system_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/server/server.py:323\u001b[0m, in \u001b[0;36mSyncServer._step\u001b[0;34m(self, user_id, agent_id, input_message, timestamp)\u001b[0m\n\u001b[1;32m    321\u001b[0m step_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     step_response \u001b[38;5;241m=\u001b[39m \u001b[43mletta_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_input_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_verify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_verify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dicts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_streaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     step_response\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m    333\u001b[0m     heartbeat_request \u001b[38;5;241m=\u001b[39m step_response\u001b[38;5;241m.\u001b[39mheartbeat_request\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/agent.py:815\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, user_message, first_message, first_message_retry_limit, skip_verify, return_dicts, recreate_message_timestamp, stream, timestamp, inner_thoughts_in_kwargs, ms)\u001b[0m\n\u001b[1;32m    813\u001b[0m response_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    814\u001b[0m response_message\u001b[38;5;241m.\u001b[39mmodel_copy()  \u001b[38;5;66;03m# TODO why are we copying here?\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m all_response_messages, heartbeat_request, function_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_ai_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO this is kind of hacky, find a better way to handle this\u001b[39;49;00m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the only time we set up message creation ahead of time is when streaming is on\u001b[39;49;00m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_message_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Step 6: extend the message history\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/agent.py:629\u001b[0m, in \u001b[0;36mAgent._handle_ai_response\u001b[0;34m(self, response_message, override_tool_call_id, response_message_id)\u001b[0m\n\u001b[1;32m    625\u001b[0m         function_args[name] \u001b[38;5;241m=\u001b[39m spec[name](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunction_args[name])\n\u001b[1;32m    627\u001b[0m function_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# need to attach self to arg since it's dynamically linked\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m function_response \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_to_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_search\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_search_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchival_memory_search\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# with certain functions we rely on the paging mechanism to handle overflow\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     truncate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<string>:12\u001b[0m, in \u001b[0;36mconsider_candidate\u001b[0;34m(self, name)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/client/client.py:1734\u001b[0m, in \u001b[0;36mLocalClient.send_message\u001b[0;34m(self, message, role, agent_id, agent_name, stream_steps, stream_tokens, include_full_message)\u001b[0m\n\u001b[1;32m   1732\u001b[0m     usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39msystem_message(user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_id, agent_id\u001b[38;5;241m=\u001b[39magent_id, message\u001b[38;5;241m=\u001b[39mmessage)\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1734\u001b[0m     usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRole \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrole\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/server/server.py:541\u001b[0m, in \u001b[0;36mSyncServer.user_message\u001b[0;34m(self, user_id, agent_id, message, timestamp)\u001b[0m\n\u001b[1;32m    533\u001b[0m         message \u001b[38;5;241m=\u001b[39m Message(\n\u001b[1;32m    534\u001b[0m             user_id\u001b[38;5;241m=\u001b[39muser_id,\n\u001b[1;32m    535\u001b[0m             agent_id\u001b[38;5;241m=\u001b[39magent_id,\n\u001b[1;32m    536\u001b[0m             role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    537\u001b[0m             text\u001b[38;5;241m=\u001b[39mpackaged_user_message,\n\u001b[1;32m    538\u001b[0m         )\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# Run the agent state forward\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackaged_user_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usage\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/server/server.py:323\u001b[0m, in \u001b[0;36mSyncServer._step\u001b[0;34m(self, user_id, agent_id, input_message, timestamp)\u001b[0m\n\u001b[1;32m    321\u001b[0m step_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     step_response \u001b[38;5;241m=\u001b[39m \u001b[43mletta_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_input_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_verify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_verify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dicts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_streaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     step_response\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m    333\u001b[0m     heartbeat_request \u001b[38;5;241m=\u001b[39m step_response\u001b[38;5;241m.\u001b[39mheartbeat_request\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/agent.py:804\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, user_message, first_message, first_message_retry_limit, skip_verify, return_dicts, recreate_message_timestamp, stream, timestamp, inner_thoughts_in_kwargs, ms)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit first message retry limit (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_message_retry_limit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 804\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ai_reply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_message_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_thoughts_in_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_thoughts_in_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# Step 3: check if LLM wanted to call a function\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# (if yes) Step 4: call the function\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# (if yes) Step 5: send the info on the function call and function response to LLM\u001b[39;00m\n\u001b[1;32m    813\u001b[0m response_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/agent.py:464\u001b[0m, in \u001b[0;36mAgent._get_ai_reply\u001b[0;34m(self, message_sequence, function_call, first_message, stream, inner_thoughts_in_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get response from LLM API\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# agent_state=self.agent_state,\u001b[39;49;00m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions_python\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions_python\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# hint\u001b[39;49;00m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# streaming\u001b[39;49;00m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_inferface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# putting inner thoughts in func args or not\u001b[39;49;00m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_thoughts_in_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_thoughts_in_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI call didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt return a message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/letta/llm_api/llm_api_tools.py:217\u001b[0m, in \u001b[0;36mretry_with_exponential_backoff.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Sleep for the delay\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# printd(f\"Got a rate limit error ('{http_err}') on LLM backend request, waiting {int(delay)}s then retrying...\")\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLI_WARNING_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mGot a rate limit error (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhttp_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) on LLM backend request, waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(delay)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms then retrying...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(delay)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# For other HTTP errors, re-raise the exception\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = client.send_message(\n",
    "    agent_id=recruiter_agent.id, \n",
    "    role=\"system\", \n",
    "    message=\"Run generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd187b-410c-4f31-b01a-2e8b0b57f43b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "nb_print(response.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e1c47-1d19-4021-8665-d5c74353464e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
